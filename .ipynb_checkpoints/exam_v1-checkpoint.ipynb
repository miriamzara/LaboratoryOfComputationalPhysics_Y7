{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy 4.9\n",
    "Analyze a data file \n",
    "  * Download the population of hares, lynxes and carrots at the beginning of the last century.\n",
    "    ```python\n",
    "    ! wget https://www.dropbox.com/s/3vigxoqayo389uc/populations.txt\n",
    "    ```\n",
    "\n",
    "  * Check the content by looking within the file\n",
    "  * Load the data (use an appropriate numpy method) into a 2D array\n",
    "  * Create arrays out of the columns, the arrays being (in order): *year*, *hares*, *lynxes*, *carrots* \n",
    "  * Plot the 3 populations over the years\n",
    "  * Compute the main statistical properties of the dataset (mean, std, correlations, etc.)\n",
    "  * Which species has the highest population each year?\n",
    "\n",
    "Do you feel there is some evident correlation here? [Studies](https://www.enr.gov.nt.ca/en/services/lynx/lynx-snowshoe-hare-cycle) tend to believe so.\n",
    "\n",
    "! curl -L -o populations.txt https://www.dropbox.com/s/3vigxoqayo389uc/populations.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bash 3.1\n",
    "1\\.a Make a new directory called `students` in your home. Download a csv file with the list of students of this lab from [here](https://www.dropbox.com/s/867rtx3az6e9gm8/LCP_22-23_students.csv) (use the `wget` command) and copy that to `students`. First check whether the file is already there\n",
    "\n",
    "**1\\.b** Make two new files, one containing the students belonging to PoD, the other to Physics.\n",
    "\n",
    "**1\\.c** For each letter of the alphabet, count the number of students whose surname starts with that letter. \n",
    "\n",
    "**1\\.d** Find out which is the letter with most counts.\n",
    "\n",
    "**1\\.e** Assume an obvious numbering of the students in the file (first line is 1, second line is 2, etc.), group students \"modulo 18\", i.e. 1,19,37,.. 2,20,38,.. etc. and put each group in a separate file  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "# 1.a\n",
    "mkdir students\n",
    "curl -L -O \"https://www.dropbox.com/scl/fi/bxv17nrbrl83vw6qrkiu9/LCP_22-23_students.csv?rlkey=47fakvatrtif3q3qw4q97p5b7&e=1\" \n",
    "cd students\n",
    "ls \n",
    "cp ../LCP_22-23_students.csv ./LCP_22-23_students.csv\n",
    "\n",
    "# 1.b\n",
    "head -n 1 LCP_22-23_students.csv > pod_students.csv # creates files and prints header inside\n",
    "head -n 1 LCP_22-23_students.csv > physics_students.csv\n",
    "grep \"PoD\" LCP_22-23_students.csv >> pod_students.csv # >> appends, > overwrites\n",
    "grep \"Physics\" LCP_22-23_students.csv >> physics_students.csv\n",
    "\n",
    "# 1.c, 1.d\n",
    "\n",
    "# chmod +x my_script.sh\n",
    "# ./my_script.sh\n",
    "max_counts_letter=A\n",
    "max_counts=$(grep -c \"^$max_counts_letter\" LCP_22-23_students.csv) # ^ = starting with\n",
    "for letter in {A..Z}; do\n",
    "    counts=$(grep -c \"^$letter\" LCP_22-23_students.csv)\n",
    "    echo \"$letter, $counts\"\n",
    "    if [ $counts -gt $max_counts ]; then\n",
    "        max_counts=$counts # careful do NOT add spaces!\n",
    "        max_counts_letter=$letter\n",
    "    fi\n",
    "done\n",
    "echo \"done.\"\n",
    "echo \"The letter with max counts is $max_counts_letter with $max_counts\"\n",
    "\n",
    "# 1.e\n",
    "for (( i=0; i<18; i++ )); do\n",
    "    head -n 1 \"LCP_22-23_students.csv\" > \"${i}_mod_18_students.csv\" # use {}\n",
    "done\n",
    "index=0 #skip the first line, which is the header\n",
    "while IFS= read -r line; do\n",
    "    if [ $index -gt 0 ]; then\n",
    "        module=$(( index % 18 ))\n",
    "        echo \"$line\" >> \"${module}_mod_18_students.csv\" #>> append, use {}\n",
    "    fi\n",
    "    (( index++ )) # dont use $\n",
    "done < LCP_22-23_students.csv # specify where to read from\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 7.1\\. Kernel Density Estimate (done)\n",
    "\n",
    "Produce a KDE for a given distribution (by hand, not using seaborn!):\n",
    "\n",
    "* Fill a numpy array, x,  of length **N** (with N=O(100)) with a variable normally distributed, with a given mean a standard deviation **$\\sigma$**.\n",
    "* Fill an histogram in pyplot taking properly care about the aesthetic\n",
    "   * use a meaningful number of bins\n",
    "   * set a proper y axis label\n",
    "   * set proper value of y axis major ticks labels (e.g. you want to display only integer labels)\n",
    "   * display the histograms as data points with errors (the error being the poisson uncertainty)\n",
    "* for every element of x, create a gaussian with the mean corresponding the element value and the std as a parameter that can be tuned. The std default value should be:\n",
    "$$ \\alpha * \\sigma * N ^{-\\frac{1}{5.}} $$\n",
    "\n",
    "The KDE is a procedure used to smooth the histogram. The parameter $\\alpha$ tunes the bandwidth of the individual gaussians:\n",
    "- if choosen too small, data artifacts will remain visible (the underlying distribution is under-smoothed)\n",
    "- if choosen too large, the real pdf will be obscured.\n",
    "\n",
    "Start from a value of $\\alpha = 1.06$ and then tune.\n",
    "\n",
    "\n",
    "you can use the scipy function `stats.norm()` for that.\n",
    "* In a separate plot (to be placed beside the original histogram), plot all the gaussian functions so obtained\n",
    "* Sum (with np.sum()) all the gaussian functions and normalize the result such that the integral matches the integral of the original histogram. For that you could use the `scipy.integrate.trapz()` method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms 9.2 \n",
    "Curve fitting of temperature in Alaska\n",
    "\n",
    "The temperature extremes in Alaska for each month, starting in January, are given by (in degrees Celcius):\n",
    "\n",
    "max:  17,  19,  21,  28,  33,  38, 37,  37,  31,  23,  19,  18\n",
    "\n",
    "min: -62, -59, -56, -46, -32, -18, -9, -13, -25, -46, -52, -58\n",
    "\n",
    "* Plot these temperature extremes.\n",
    "* Define a function that can describe min and max temperatures. \n",
    "* Fit this function to the data with scipy.optimize.curve_fit().\n",
    "* Plot the result. Is the fit reasonable? If not, why?\n",
    "* Is the time offset for min and max temperatures the same within the fit accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montecarlo 10.1\n",
    "\n",
    "**Radioactive decay chain**\n",
    "\n",
    "${\\rm Tl}^{208}$ decays to ${\\rm Pb}^{208}$ with a half-life of 3.052 minutes. Suppose to start with a sample of 1000 Thallium atoms and 0 of Lead atoms.\n",
    "\n",
    "* Take steps in time of 1 second and at each time-step decide whether each Tl atom has decayed or not, accordingly to the probability $p(t)=1-2^{-t/\\tau}$. Subtract the total number of Tl atoms that decayed at each step from the Tl sample and add them to the Lead one. Plot the evolution of the two sets as a function of time  \n",
    "* Repeat the exercise by means of the inverse transform method: draw 1000 random numbers from the non-uniform probability distribution $p(t)=2^{-t/\\tau}\\frac{\\ln 2}{\\tau}$ to represent the times of decay of the 1000 Tl atoms. Make a plot showing the number of atoms that have not decayed as a function of time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
